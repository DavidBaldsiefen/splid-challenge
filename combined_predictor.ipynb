{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 19:47:13.426823: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-07 19:47:13.426903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-07 19:47:13.496382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-07 19:47:13.645276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-07 19:47:14.975076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from base import datahandler, prediction_models, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 19:47:17.697211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:1c:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-07 19:47:17.948321: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:1c:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-07 19:47:17.948405: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:1c:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-07 19:47:17.950120: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:1c:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-07 19:47:17.950193: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:1c:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-07 19:47:17.950243: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:1c:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-07 19:47:19.986841: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:1c:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-07 19:47:19.986920: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:1c:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-07 19:47:19.986930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-07 19:47:19.986989: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:1c:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-07 19:47:19.987008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4575 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:1c:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "ew_localizer = tf.keras.models.load_model(\"models/ew_localizer.hdf5\")\n",
    "ns_localizer = tf.keras.models.load_model(\"models/ns_localizer.hdf5\")\n",
    "ew_ns_classifier = tf.keras.models.load_model(\"models/ew_ns_classifier.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some validation data\n",
    "challenge_data_dir = Path('dataset/phase_1/')\n",
    "data_dir = challenge_data_dir / \"train\"\n",
    "labels_dir = challenge_data_dir / 'train_labels.csv'\n",
    "\n",
    "split_dataframes = datahandler.load_and_prepare_dataframes(data_dir, labels_dir)\n",
    "train_keys = list(split_dataframes.keys())[150:]\n",
    "val_keys = list(split_dataframes.keys())[150:]\n",
    "some_dataframes = {df_k : split_dataframes[df_k] for df_k in val_keys}\n",
    "\n",
    "input_features_reduced = ['Eccentricity', 'Semimajor Axis (m)', 'Inclination (deg)', 'RAAN (deg)',\n",
    "       'Argument of Periapsis (deg)', 'True Anomaly (deg)', 'Latitude (deg)',\n",
    "       'Longitude (deg)', 'Altitude (m)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================Creating Dataset=========================\n",
      "Seed: 69\n",
      "nTrain: 20 nVal: 0 (1.00)\n",
      "Padding: False\n",
      "Scaling: True\n",
      "Horizons: 40-40 @ stride 1\n",
      "Sin-Transformed features: ['True Anomaly (deg)', 'Argument of Periapsis (deg)']\n",
      "Labels: ['EW_Node_Location', 'NS_Node_Location']\n",
      "=========================Finished Dataset=========================\n",
      "1312/1312 [==============================] - 3s 3ms/step\n",
      "1312/1312 [==============================] - 3s 3ms/step\n",
      "24\n",
      "    ObjectID  TimeIndex  Location_EW  Location_NS\n",
      "0         28          0            1            1\n",
      "1         59          0            1            1\n",
      "2         63          0            1            1\n",
      "3         67          0            1            1\n",
      "4         86          0            1            1\n",
      "5         97          0            1            1\n",
      "6        111          0            1            1\n",
      "7        111        442            1            0\n",
      "8        123          0            1            1\n",
      "9        124          0            1            1\n",
      "10       124       1725            1            0\n",
      "11       151          0            1            1\n",
      "12       178          0            1            1\n",
      "13       178       1888            1            0\n",
      "14       178       1889            1            0\n",
      "15       197          0            1            1\n",
      "16       205          0            1            1\n",
      "17       208          0            1            1\n",
      "18       210          0            1            1\n",
      "19       226          0            1            1\n"
     ]
    }
   ],
   "source": [
    "# start with localization\n",
    "ds_gen = datahandler.DatasetGenerator(split_df=some_dataframes,\n",
    "                                      input_features=input_features_reduced,\n",
    "                                      label_features=['EW_Node_Location', 'NS_Node_Location'],\n",
    "                                      train_val_split=1.0,\n",
    "                                      stride=1,\n",
    "                                      input_stride=1,\n",
    "                                      padding=False,\n",
    "                                      input_history_steps=40,\n",
    "                                      input_future_steps=40,\n",
    "                                      seed=69)\n",
    "test_ds = ds_gen.get_datasets(256, label_features=['EW_Node_Location', 'NS_Node_Location'], shuffle=False, keep_identifier=True)\n",
    "\n",
    "inputs = np.concatenate([element for element in test_ds.map(lambda x,y,z: x).as_numpy_iterator()])\n",
    "identifiers = np.concatenate([element for element in test_ds.map(lambda x,y,z: z).as_numpy_iterator()])\n",
    "\n",
    "# get predictions\n",
    "preds_ew = ew_localizer.predict(inputs)\n",
    "preds_ew_argmax = np.argmax(preds_ew, axis=1)\n",
    "preds_ns = ns_localizer.predict(inputs)\n",
    "preds_ns_argmax = np.argmax(preds_ns, axis=1)\n",
    "\n",
    "df = pd.DataFrame(np.concatenate([identifiers.reshape(-1,2)], axis=1), columns=['ObjectID', 'TimeIndex'], dtype=np.int32)\n",
    "df[f'Location_EW'] = preds_ew_argmax\n",
    "df[f'Location_NS'] = preds_ns_argmax\n",
    "\n",
    "# add initial node prediction\n",
    "for obj_id in list(df[\"ObjectID\"].unique()):\n",
    "    df = df.sort_index()\n",
    "    df.loc[-1] = [int(obj_id), 0, 1, 1] # objid, timeindex, Location_EW, Location_NS\n",
    "    df.index = df.index + 1\n",
    "    df = df.sort_index()\n",
    "\n",
    "df_locs = df.loc[(df['Location_EW'] == 1) | (df['Location_NS'] == 1)]\n",
    "df_locs = df_locs.sort_values(['ObjectID', 'TimeIndex']).reset_index(drop=True)\n",
    "print(len(df_locs))\n",
    "print(df_locs.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================Creating Dataset=========================\n",
      "Seed: 69\n",
      "nTrain: 20 nVal: 0 (1.00)\n",
      "Padding: True\n",
      "Scaling: True\n",
      "Horizons: 40-40 @ stride 2\n",
      "Sin-Transformed features: ['True Anomaly (deg)', 'Argument of Periapsis (deg)']\n",
      "Labels: ['EW_encoded', 'NS_encoded']\n",
      "=========================Finished Dataset=========================\n",
      "1361/1361 [==============================] - 7s 5ms/step\n",
      "   ObjectID  TimeIndex Direction Node Type\n",
      "0        28          0        EW   SS   CK\n",
      "1        28          0        NS   SS   CK\n",
      "2        28          1        EW   SS   CK\n",
      "3        28          1        NS   SS   CK\n",
      "4        28          2        EW   SS   CK\n"
     ]
    }
   ],
   "source": [
    "# now we need to get label predictions on those locations\n",
    "ds_gen = datahandler.DatasetGenerator(split_df=some_dataframes,\n",
    "                                      input_features=input_features_reduced,\n",
    "                                      label_features=['EW', 'NS'],\n",
    "                                      train_val_split=1.0,\n",
    "                                      stride=1,\n",
    "                                      input_stride=2, #!\n",
    "                                      padding=True,\n",
    "                                      input_history_steps=40,\n",
    "                                      input_future_steps=40,\n",
    "                                      seed=69)\n",
    "test_ds = ds_gen.get_datasets(256, label_features=['EW', 'NS'], shuffle=False, keep_identifier=True)\n",
    "\n",
    "inputs = np.concatenate([element for element in test_ds.map(lambda x,y,z: x).as_numpy_iterator()])\n",
    "#labels = np.concatenate([element['EW_Node_Location'] for element in ds.map(lambda x,y,z: y).as_numpy_iterator()])\n",
    "identifiers = np.concatenate([element for element in test_ds.map(lambda x,y,z: z).as_numpy_iterator()])\n",
    "\n",
    "\n",
    "# get predictions\n",
    "preds = ew_ns_classifier.predict(inputs)\n",
    "sub_dfs = []\n",
    "for ft_idx, feature in enumerate(['EW', 'NS']):\n",
    "    sub_df = pd.DataFrame(np.concatenate([identifiers.reshape(-1,2)], axis=1), columns=['ObjectID', 'TimeIndex'], dtype=np.int32)\n",
    "    sub_df['Direction'] = feature\n",
    "    preds_argmax = np.argmax(preds[ft_idx], axis=1)\n",
    "    sub_df[f'{feature}_Decoded'] = ds_gen.combined_label_encoder.inverse_transform(preds_argmax)\n",
    "    sub_df[['Node', 'Type']] = sub_df[f'{feature}_Decoded'].str.split('-', expand=True)\n",
    "    sub_df = sub_df.drop([f'{feature}_Decoded'], axis='columns')\n",
    "    sub_dfs.append(sub_df)\n",
    "\n",
    "# Add direction column\n",
    "df_classes = pd.concat(sub_dfs).sort_values(['ObjectID', 'TimeIndex']).reset_index(drop=True)\n",
    "# For timeindex 0, the node is always SS\n",
    "df_classes.loc[df_classes['TimeIndex'] == 0, 'Node'] = 'SS'\n",
    "\n",
    "print(df_classes.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 59, 63, 67, 86, 97, 111, 123, 124, 151, 178, 197, 205, 208, 210, 226, 228, 238, 243, 246]\n",
      "[28, 59, 63, 67, 86, 97, 111, 123, 124, 151, 178, 197, 205, 208, 210, 226, 228, 238, 243, 246]\n",
      "   ObjectID  TimeIndex Direction Node Type\n",
      "0        28          0        EW   SS   CK\n",
      "1        28          0        NS   SS   CK\n",
      "2        59          0        EW   SS   CK\n",
      "3        59          0        NS   SS   CK\n",
      "4        63          0        EW   SS   CK\n"
     ]
    }
   ],
   "source": [
    "# Combine the classifications with the node locations\n",
    "df_merged = df_locs.merge(df_classes, how='left', on = ['ObjectID', 'TimeIndex'])\n",
    "df_reduced = df_merged[((df_merged['Location_EW'] == 1) & (df_merged['Direction'] == 'EW') | (df_merged['Location_NS'] == 1) & (df_merged['Direction'] == 'NS'))]\n",
    "df_reduced = df_reduced.drop(['Location_EW'], axis='columns')\n",
    "df_reduced = df_reduced.drop(['Location_NS'], axis='columns')\n",
    "print(df_reduced.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.77\n",
      "Recall: 0.40\n",
      "F2: 0.45\n",
      "RMSE: 0.0\n",
      "TP: 34 FP: 10 FN: 50\n"
     ]
    }
   ],
   "source": [
    "# Finally, perform the evaluation\n",
    "ground_truth = pd.read_csv(challenge_data_dir / 'train_labels.csv')\n",
    "ground_truth = ground_truth[ground_truth['ObjectID'].isin(list(df_reduced[\"ObjectID\"].unique()))].copy()\n",
    "\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth, df_reduced)\n",
    "precision, recall, f2, rmse, total_tp, total_fp, total_fn = evaluator.score()\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F2: {f2:.2f}')\n",
    "print(f'RMSE: {rmse:.4}')\n",
    "print(f'TP: {total_tp} FP: {total_fp} FN: {total_fn}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splid-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

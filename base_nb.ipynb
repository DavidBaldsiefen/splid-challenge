{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from base import utils, datahandler, prediction_models, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths\n",
    "challenge_data_dir = Path('dataset/')\n",
    "data_dir = challenge_data_dir / \"train\"\n",
    "labels_dir = challenge_data_dir / 'train_labels.csv'\n",
    "\n",
    "split_dataframes = datahandler.load_and_prepare_dataframes(data_dir, labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SS-CK', 'SS-HK', 'SS-EK']\n",
      "['SS-CK', 'SS-HK']\n",
      "['SS-NK', 'IK-CK', 'SS-CK', 'SS-HK', 'IK-HK']\n",
      "['SS-NK', 'IK-CK', 'SS-HK']\n"
     ]
    }
   ],
   "source": [
    "# get all labels in the dataframes\n",
    "keys_list = list(split_dataframes.keys())\n",
    "random.Random(69).shuffle(keys_list) # shuffle, but with a seed for reproducability\n",
    "split_idx = int(len(keys_list) * 0.8)\n",
    "train_keys = keys_list[:split_idx]\n",
    "train_labels_EW = []\n",
    "train_labels_NS = []\n",
    "val_keys = keys_list[split_idx:]\n",
    "val_labels_EW = []\n",
    "val_labels_NS = []\n",
    "for key in train_keys:\n",
    "    train_labels_EW += list(split_dataframes[key]['EW'].unique())\n",
    "    train_labels_NS += list(split_dataframes[key]['NS'].unique())\n",
    "for key in val_keys:\n",
    "    val_labels_EW += list(split_dataframes[key]['EW'].unique())\n",
    "    val_labels_NS += list(split_dataframes[key]['NS'].unique())\n",
    "train_labels_EW = list(dict.fromkeys(train_labels_EW))\n",
    "train_labels_NS = list(dict.fromkeys(train_labels_NS))\n",
    "val_labels_EW = list(dict.fromkeys(val_labels_EW))\n",
    "val_labels_NS = list(dict.fromkeys(val_labels_NS))\n",
    "print(train_labels_EW)\n",
    "print(val_labels_EW)\n",
    "print(train_labels_NS)\n",
    "print(val_labels_NS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset from 15 train and 3 val objects\n",
      "Created datasets with seed 69\n"
     ]
    }
   ],
   "source": [
    "ds_gen = datahandler.DatasetGenerator(split_df=split_dataframes, train_val_split=0.85, stride=1, input_steps=5, seed=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model #1\n",
      "Training model #2\n",
      "EW results:\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9993\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.2679 - accuracy: 0.5413\n",
      "NS results:\n",
      "255/255 [==============================] - 1s 3ms/step - loss: 0.0358 - accuracy: 0.9882\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 10.4166 - accuracy: 0.2598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10.416610717773438, 0.25982198119163513]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_EW, val_EW = ds_gen.get_datasets(128, label_features=['EW'], shuffle=True)\n",
    "train_NS, val_NS = ds_gen.get_datasets(128, label_features=['NS'], shuffle=True)\n",
    "#dense_model_ew = prediction_models.Dense_NN(train_EW, dense_layers=[512,256,32], l2_reg=0.0001, mixed_dropout=0.2)\n",
    "#dense_model_ns = prediction_models.Dense_NN(train_EW, dense_layers=[512,256,32], l2_reg=0.0001, mixed_dropout=0.2)\n",
    "dense_model_ew = prediction_models.CNN(train_EW, conv_layers=[64,64,64], l2_reg=0.00, mixed_dropout=0.0, seed=0)\n",
    "dense_model_ns = prediction_models.CNN(train_EW, conv_layers=[64,64,64], l2_reg=0.00, mixed_dropout=0.0, seed=0)\n",
    "print(\"Training model #1\")\n",
    "hist_ew = dense_model_ew.fit(train_EW, val_ds=val_EW, epochs=10, verbose=0, plot_hist=False) # TODO: somehow this sometimes has save_best_only enabled?? even though its only enabled in 2nd fit? ... do inherited models somehow modify the mother class?!?\n",
    "print(\"Training model #2\")\n",
    "hist_ns = dense_model_ns.fit(train_NS, val_ds=val_NS, epochs=10, verbose=0, plot_hist=False)\n",
    "print(\"EW results:\")\n",
    "dense_model_ew.model.evaluate(train_EW)\n",
    "dense_model_ew.model.evaluate(val_EW)\n",
    "print(\"NS results:\")\n",
    "dense_model_ns.model.evaluate(train_NS)\n",
    "dense_model_ns.model.evaluate(val_NS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find out if we can directly train for precision/challenge metrics\n",
    "# TODO: maybe it makes sense to look at values before and after the node? Given that we are trying to detect changes...\n",
    "# TODO: check if train_NS even contains all the labels in val_NS... in general, train should contain all labels\n",
    "# TODO: Make sure labelencoder gets saved\n",
    "# TODO: Support models with different label features in the submission pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_submission_evaluation(ds_EW, ds_NS, label_encoder, model_ew, model_ns):\n",
    "    # This approach is dirty, but serves the purpose well\n",
    "    # inputs and identifiers are identical for both sets\n",
    "    inputs = np.concatenate([element for element in ds_EW.map(lambda x,y,z: x).as_numpy_iterator()])\n",
    "    labels_EW = np.concatenate([element for element in ds_EW.map(lambda x,y,z: y).as_numpy_iterator()]).reshape(-1,1) # turn (n,) to (n,1)\n",
    "    labels_NS = np.concatenate([element for element in ds_NS.map(lambda x,y,z: y).as_numpy_iterator()]).reshape(-1,1)\n",
    "    identifiers = np.concatenate([element for element in ds_EW.map(lambda x,y,z: z).as_numpy_iterator()])#[:,0,:]\n",
    "    #print(inputs.shape, labels_EW.shape, labels_NS.shape, identifiers.shape)\n",
    "    # perform evaluation and prediction\n",
    "    model_ew.evaluate(inputs, labels_EW)\n",
    "    model_ns.evaluate(inputs, labels_NS)\n",
    "    preds_ew = model_ew.predict(inputs)\n",
    "    preds_ew_argmax = np.argmax(preds_ew, axis=1).reshape(-1,1)\n",
    "    preds_ns = model_ns.predict(inputs)\n",
    "    preds_ns_argmax = np.argmax(preds_ns, axis=1).reshape(-1,1)\n",
    "    # create dataframe that contains all the necessary data\n",
    "    df = pd.DataFrame(np.concatenate([identifiers.reshape(-1,2), labels_EW, preds_ew_argmax, labels_NS, preds_ns_argmax], axis=1), columns=['ObjectID', 'TimeIndex', 'EW', 'Predicted_EW', 'NS', 'Predicted_NS'], dtype=np.int32)\n",
    "    # now remove unnecessary columns, decode prediction\n",
    "    df['Predicted_EW'] = label_encoder.inverse_transform(df['Predicted_EW'])\n",
    "    df['Predicted_NS'] = label_encoder.inverse_transform(df['Predicted_NS'])\n",
    "    df['EW'] = label_encoder.inverse_transform(df['EW'])\n",
    "    df['NS'] = label_encoder.inverse_transform(df['NS'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_EW, val_EW = ds_gen.get_datasets(128, label_features=['EW'], shuffle=False, with_identifiers=True)\n",
    "train_NS, val_NS = ds_gen.get_datasets(128, label_features=['NS'], shuffle=False, with_identifiers=True)\n",
    "ground_truth = pd.read_csv(challenge_data_dir / 'train_labels.csv')\n",
    "ground_truth_train = ground_truth[ground_truth['ObjectID'].isin(map(int, ds_gen.train_keys))].copy()\n",
    "ground_truth_val = ground_truth[ground_truth['ObjectID'].isin(map(int, ds_gen.val_keys))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019/1019 [==============================] - 3s 2ms/step - loss: 0.0028 - accuracy: 0.9993\n",
      "1019/1019 [==============================] - 3s 2ms/step - loss: 0.0358 - accuracy: 0.9882\n",
      "1019/1019 [==============================] - 2s 2ms/step\n",
      "1019/1019 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Smoothing: 100%|██████████| 15/15 [00:17<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for the train set: 0.45\n",
      "Recall for the train set: 0.62\n",
      "F2 for the train set: 0.58\n",
      "RMSE for the train set: 0.37\n"
     ]
    }
   ],
   "source": [
    "df_train=perform_submission_evaluation(train_EW, train_NS, ds_gen.combined_label_encoder, dense_model_ew.model, dense_model_ns.model)\n",
    "smoothed_df_train = utils.smooth_predictions(df_train, past_steps=3, fut_steps=4)\n",
    "train_results = utils.convert_classifier_output(smoothed_df_train).sort_values(['ObjectID']).reset_index(drop=True)\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_train, train_results)\n",
    "precision, recall, f2, rmse = evaluator.score()\n",
    "print(f'Precision for the train set: {precision:.2f}')\n",
    "print(f'Recall for the train set: {recall:.2f}')\n",
    "print(f'F2 for the train set: {f2:.2f}')\n",
    "print(f'RMSE for the train set: {rmse:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 1s 3ms/step - loss: 3.2679 - accuracy: 0.5413\n",
      "204/204 [==============================] - 1s 3ms/step - loss: 10.4166 - accuracy: 0.2598\n",
      "204/204 [==============================] - 1s 3ms/step\n",
      "204/204 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Smoothing: 100%|██████████| 3/3 [00:03<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for the val set: 0.02\n",
      "Recall for the val set: 0.40\n",
      "F2 for the val set: 0.09\n",
      "RMSE for the val set: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_val=perform_submission_evaluation(val_EW, val_NS, ds_gen.combined_label_encoder, dense_model_ew.model, dense_model_ns.model)\n",
    "smoothed_df_val = utils.smooth_predictions(df_val)\n",
    "val_results = utils.convert_classifier_output(smoothed_df_val).sort_values(['ObjectID']).reset_index(drop=True)\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_val, val_results)\n",
    "precision, recall, f2, rmse = evaluator.score()\n",
    "print(f'Precision for the val set: {precision:.2f}')\n",
    "print(f'Recall for the val set: {recall:.2f}')\n",
    "print(f'F2 for the val set: {f2:.2f}')\n",
    "print(f'RMSE for the val set: {rmse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset from 18 train and 0 val objects\n",
      "Created datasets with seed 42\n",
      "Training model #1\n",
      "Training model #2\n",
      "Evaluation:\n",
      "306/306 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "306/306 [==============================] - 1s 2ms/step - loss: 0.0524 - accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "ds_gen_noval = datahandler.DatasetGenerator(split_df=split_dataframes, train_val_split=1.0, stride=1, input_steps=15)\n",
    "train_EW, train_NS = ds_gen_noval.get_datasets(128, shuffle=True)\n",
    "dense_model_ew = prediction_models.Dense_NN(train_EW, dense_layers=[128,128], l2_reg=0.0002, mixed_dropout=0.1)\n",
    "dense_model_ns = prediction_models.Dense_NN(train_EW, dense_layers=[128,128], l2_reg=0.0002, mixed_dropout=0.1)\n",
    "print(\"Training model #1\")\n",
    "hist_ew = dense_model_ew.fit(train_EW, epochs=40, verbose=0, plot_hist=False) # TODO: somehow this sometimes has save_best_only enabled?? even though its only enabled in 2nd fit? ... do inherited models somehow modify the mother class?!?\n",
    "print(\"Training model #2\")\n",
    "hist_ns = dense_model_ns.fit(train_NS, epochs=40, verbose=0, plot_hist=False)\n",
    "print(\"Evaluation:\")\n",
    "eval_res = dense_model_ew.model.evaluate(train_EW)\n",
    "eval_res = dense_model_ns.model.evaluate(train_NS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David\\anaconda3\\envs\\splid\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "dense_model_ew.model.save('trained_models/dense_model_ew.h5')\n",
    "dense_model_ns.model.save('trained_models/dense_model_ns.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 1s 2ms/step\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 15, 16)]          0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 240)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               30848     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49940 (195.08 KB)\n",
      "Trainable params: 49940 (195.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('trained_models/dense_model_ew.h5')\n",
    "new_model.predict(train_EW)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for the val set: 0.73\n",
      "Recall for the val set: 0.65\n",
      "F2 for the val set: 0.66\n",
      "RMSE for the val set: 0.46\n"
     ]
    }
   ],
   "source": [
    "ground_truth = pd.read_csv(challenge_data_dir / 'train_labels.csv')\n",
    "predictions_final = pd.read_csv('submission/submission.csv')\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth, predictions_final)\n",
    "precision, recall, f2, rmse = evaluator.score()\n",
    "print(f'Precision for the val set: {precision:.2f}')\n",
    "print(f'Recall for the val set: {recall:.2f}')\n",
    "print(f'F2 for the val set: {f2:.2f}')\n",
    "print(f'RMSE for the val set: {rmse:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
